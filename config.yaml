# LLM Configuration
# This file contains settings for the LLM provider and model

llm:
  # API settings
  # api_base: "http://localhost:5000/v1"  # Optional: set to None or remove for default provider endpoint
  # provider: "vllm"
  # model_name: "vllm/vllm"

  api_base: "https://api.z.ai/api/coding/paas/v4"  # Optional: set to None or remove for default provider endpoint
  provider: "zai"
  model_name: "zai/glm-4.7"

  # Optional: Override the default API key environment variable
  # If not set, will auto-determine based on provider name (e.g., VLLM_API_KEY, ZAI_API_KEY)
  # api_key_env: "VLLM_API_KEY"

  # Model capabilities
  max_completion_tokens: 8196
  context_size: 262144

  # Media support
  support_image: true
  support_audio_input: false
  support_audio_output: false

  # Thinking mode (for models that support it)
  thinking:
    level: "high"
    include_thoughts: true

# Other settings
debug: false

# ====== Provider Examples ======
#
# Native Mirascope providers (no api_base needed):
#   provider: "anthropic"     # uses ANTHROPIC_API_KEY
#   provider: "openai"        # uses OPENAI_API_KEY
#   provider: "google"         # uses GOOGLE_API_KEY
#   provider: "ollama"        # uses OLLAMA_BASE_URL
#   provider: "mlx"           # no API key needed
#
# OpenAI-compatible providers (requires api_base):
#   provider: "vllm"          # uses VLLM_API_KEY or OPENAI_API_KEY
#   api_base: "http://localhost:8000/v1"
#
#   provider: "zai"           # Z.AI GLM models
#   api_base: "https://api.z.ai/api/coding/paas/v4"
#   model_name: "zai/GLM-4.7"  # or GLM-4.5-air
